# LipSync
This project presents an end-to-end system for automatic sign language recognition using deep learning techniques. The system is designed to preprocess sign language videos, align them with corresponding textual annotations, and train a deep neural network for recognition. The data pipeline integrates TensorFlow and OpenCV for efficient data loading and preprocessing. A Conv3D-based architecture is proposed, followed by Bidirectional LSTM layers for sequence modelling. The model is trained with a custom CTC loss function and optimized using the Adam optimizer. Additionally, training is augmented with learning rate scheduling and checkpointing. The system is evaluated on a dataset containing diverse sign language gestures and achieves promising results. Furthermore, real-time predictions on test videos demonstrate the system's effectiveness in accurately recognizing sign language gestures. Overall, this project showcases the feasibility and efficacy of deep learning in real-world applications, particularly in assisting individuals with hearing impairments through sign language recognition technology.

Please contact the owners to get access to training datasets and model checkpoints used
